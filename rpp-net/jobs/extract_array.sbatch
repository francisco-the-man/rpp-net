#!/bin/bash
###############################################################################
#  SLURM  JOBâ€‘ARRAY  â€”  RPP network & homophily feature extraction
#
#  One task  =  one CSV chunk (â‰ˆÂ 1Â ~Â n DOIs) created by split_targets.py
#  Each task:
#     â€¢ creates a disposable venv in $TMPDIR  (fast local storage)
#     â€¢ installs Python deps             (cached wheels â†’ quick)
#     â€¢ calls  src/run_worker.py  with   --chunk_id = $SLURM_ARRAY_TASK_ID
#
#  Output:
#     data/features/results_chunk_<ID>.csv
#     data/networks_raw/<doi>.json
#     logs/<JOBID>_<ARRAYID>.out   (stdout / stderr)
###############################################################################

#SBATCH --job-name=rpp_net
#SBATCH --array=0-10                     # 100 chunks â†’ adjust if n_chunks differs
#SBATCH --cpus-per-task=4               # aiohttp concurrency lives on threads
#SBATCH --mem=8G
#SBATCH --time=02:00:00
#SBATCH --output=logs/%A_%a.out          # %A = jobID, %a = array index
# SBATCH --gres=       # comment in if GPUs are ever needed

set -euo pipefail
echo "ðŸŸ¢  SLURM task $SLURM_ARRAY_TASK_ID starting on $(hostname) at $(date)"


VENV_DIR="$TMPDIR/rpp_net_env"
python3 -m venv "$VENV_DIR"
source "$VENV_DIR/bin/activate"
pip install --quiet -r requirements.txt   # uses cached wheels after first task
pip install --quiet python-louvain

export OPENALEX_API_KEY="averylou@stanford.edu"

CHUNK_ID=$(printf "%02d" "${SLURM_ARRAY_TASK_ID}")
python src/run_worker.py \
    --chunk_id "$CHUNK_ID" \
    --max_depth 2 \
    --max_nodes 1000 \
    --n_concurrent 5

echo "âœ…  SLURM task $SLURM_ARRAY_TASK_ID finished at $(date)"